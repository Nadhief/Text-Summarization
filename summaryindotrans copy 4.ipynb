{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 15687\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====== 1. Load Dataset ======\n",
    "dataset = load_dataset(\"SEACrowd/liputan6\", trust_remote_code=True)\n",
    "train_data = dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
    "val_data = dataset[\"validation\"].shuffle(seed=42).select(range(100))\n",
    "test_data = dataset[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "def custom_tokenizer(text, vocab, max_len=512):\n",
    "    # Tokenize the input text\n",
    "    tokens = text.split()[:max_len]  # Truncate to max_len\n",
    "    # Map tokens to vocab IDs, default to <OOV> if not in vocab\n",
    "    token_ids = [vocab.get(token, vocab[\"<OOV>\"]) for token in tokens]\n",
    "    # Pad to max_len\n",
    "    padding = [vocab[\"<OOV>\"]] * (max_len - len(token_ids))\n",
    "    return token_ids + padding\n",
    "\n",
    "\n",
    "# Create a custom vocabulary with only <OOV>\n",
    "vocab = {\"<OOV>\": 0}\n",
    "\n",
    "# Update vocabulary with unique words from train_data\n",
    "index = 1\n",
    "for data in train_data[\"document\"]:\n",
    "    for word in data.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Original Text: Liputan6 . com , Jombang : Maman Sugianto alias Sugik , yang masih berstatus terdakwa kasus pembunuhan Asrori , ditangguhkan penahanannya sejak Senin ( 24/11 ) malam . Penangguhan penahanannya ini dimanfaatkan Sugik untuk berkunjung ke rumah sanak saudaranya . Kedatangan Sugik disambut penuh haru bahkan sang bibi langsung jatuh pingsan . Sugik bebas bersyarat dari Lembaga Pemasyarakatan Jombang setelah menjadi korban salah tangkap dan peradilan sesat dalam kasus pembunuhan Asrori . Terdakwa lainnya yakni Devid dan Kemat , hingga kini masih dipenjara dan tengah mengajukan peninjauan kembali ( PK ) ke Mahkamah Agung . Sugik dan istrinya berharap majelis hakim lebih bijaksana untuk membebaskan dirinya dari semua tuduhan . Sebab , ia memang merasa tak membunuh Asrori seperti yang dituduhkan polisi dan jaksa [ baca : Saksi Ahli : Terdakwa Bisa Dibebaskan ] . ( UPI/Bambang Ronggo ) .\n",
      "Tokenized IDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 11, 12, 13, 14, 15, 16, 17, 4, 18, 19, 20, 21, 22, 23, 24, 25, 2, 26, 19, 27, 28, 10, 29, 30, 31, 32, 33, 34, 2, 35, 10, 36, 37, 38, 39, 40, 41, 42, 43, 44, 2, 10, 45, 46, 47, 48, 49, 5, 50, 51, 52, 53, 54, 55, 56, 57, 58, 15, 16, 17, 2, 59, 60, 61, 62, 55, 63, 4, 64, 65, 12, 66, 55, 67, 68, 69, 70, 22, 71, 24, 31, 72, 73, 2, 10, 55, 74, 75, 76, 77, 78, 79, 29, 80, 81, 47, 82, 83, 2, 84, 4, 85, 86, 87, 88, 89, 17, 90, 11, 91, 92, 55, 93, 94, 95, 6, 96, 97, 6, 59, 98, 99, 100, 2, 22, 101, 102, 24, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence Length (Excluding Padding): 140\n",
      "Number of Paddings: 372\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2:\n",
      "Original Text: Liputan6 . com , Jakarta : Presiden Susilo Bambang Yudhoyono bersama Wakil Presiden Jusuf Kalla mengumandangkan takbir di Masjid Istiqlal , Jakarta Pusat , Jumat ( 12/10 ) . Sebelum menggemakan takbir Presiden terlebih dahulu memukul beduk sebagai tanda Puasa telah berakhir . Dalam pesannya Presiden mengingatkan kepada masyarakat untuk tidak hanya memperhatikan kaum miskin selama bulan Puasa saja . Yudhoyono juga meminta kebaikan selama puasa , seperti menjaga nafsu dan tutur kata dipertahankan hingga setahun berikutnya . ( IAN/Rahmat Supana ) .\n",
      "Tokenized IDs: [1, 2, 3, 4, 103, 6, 104, 105, 106, 107, 108, 109, 104, 110, 111, 112, 113, 114, 115, 116, 4, 103, 117, 4, 118, 22, 119, 24, 2, 120, 121, 113, 104, 122, 123, 124, 125, 126, 127, 128, 129, 130, 2, 131, 132, 104, 133, 134, 135, 29, 136, 137, 138, 139, 140, 141, 142, 128, 143, 2, 107, 144, 145, 146, 141, 147, 4, 90, 148, 149, 55, 150, 151, 152, 64, 153, 154, 2, 22, 155, 156, 24, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence Length (Excluding Padding): 83\n",
      "Number of Paddings: 429\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 3:\n",
      "Original Text: Liputan6 . com , Bandung : Saat ini pamor factory outlet atau FO yang menjadi khas Kota Bandung mulai turun dan agak bergeser dengan kehadiran distribution store alias distro . Distro juga dijadikan alternatif penjualan karena fungsinya bisa menerima titipan dari berbagai macam merek perusahaan lokal . Untuk itu , modal yang diperlukan untuk membuka distro tidak terlalu besar . Keuntungan juga baru disisihkan setelah barang terjual . Keberadaan distro berawal dari idealisme memperkuat eksistensi komunitas seperti punk , penggemar skateboard , basket , dan komunitas lain yang ditunjukkan dalam gaya berpakaian . Tidak ada yang mengetahui siapa pelopor distro di Bandung . Tetapi yang jelas , kini ratusan distro berdiri di Kota Kembang , Jakarta , dan kota-kota lain . Bahkan beberapa distro telah membuka cabang di luar negeri . ( YNI/Wendy Surya ) .\n",
      "Tokenized IDs: [1, 2, 3, 4, 157, 6, 158, 27, 159, 160, 161, 162, 163, 11, 51, 164, 165, 157, 166, 167, 55, 168, 169, 170, 171, 172, 173, 9, 174, 2, 175, 144, 176, 177, 178, 179, 180, 181, 182, 183, 47, 184, 185, 186, 187, 188, 2, 189, 190, 4, 191, 11, 192, 29, 193, 174, 136, 194, 195, 2, 196, 144, 197, 198, 50, 199, 200, 2, 201, 174, 202, 47, 203, 204, 205, 206, 90, 207, 4, 208, 209, 4, 210, 4, 55, 206, 211, 11, 212, 58, 213, 214, 2, 215, 216, 11, 217, 218, 219, 174, 114, 157, 2, 220, 11, 221, 4, 65, 222, 174, 223, 114, 165, 224, 4, 103, 4, 55, 225, 211, 2, 226, 227, 174, 129, 193, 228, 114, 229, 230, 2, 22, 231, 232, 24, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sequence Length (Excluding Padding): 136\n",
      "Number of Paddings: 376\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Contoh data dari train_data\n",
    "sample_data = train_data[\"document\"][:3]  # Ambil 3 sampel pertama untuk contoh\n",
    "\n",
    "# Tokenisasi data\n",
    "for i, text in enumerate(sample_data):\n",
    "    tokenized = custom_tokenizer(text, vocab, max_len=512)\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(\"Original Text:\", text)\n",
    "    print(\"Tokenized IDs:\", tokenized)\n",
    "    print(\"Sequence Length (Excluding Padding):\", len([t for t in tokenized if t != vocab[\"<OOV>\"]]))\n",
    "    print(\"Number of Paddings:\", len([t for t in tokenized if t == vocab[\"<OOV>\"]]))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== 2. Dataset Preparation ======\n",
    "class CustomTokenizerDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, tokenizer, vocab, max_len=512):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.articles[idx]\n",
    "        summary = self.summaries[idx]\n",
    "\n",
    "        article_ids = self.tokenizer(article, self.vocab, max_len=self.max_len)\n",
    "        summary_ids = self.tokenizer(summary, self.vocab, max_len=self.max_len)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(article_ids, dtype=torch.long),\n",
    "            \"target_ids\": torch.tensor(summary_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_dataset = CustomTokenizerDataset(\n",
    "    train_data[\"document\"], train_data[\"summary\"], custom_tokenizer, vocab\n",
    ")\n",
    "val_dataset = CustomTokenizerDataset(\n",
    "    val_data[\"document\"], val_data[\"summary\"], custom_tokenizer, vocab\n",
    ")\n",
    "test_dataset = CustomTokenizerDataset(\n",
    "    test_data[\"document\"], test_data[\"summary\"], custom_tokenizer, vocab\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== 3. Define Transformer Model ======\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, ff_dim, num_layers):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embed = self.embedding(src).permute(1, 0, 2)  # (seq_len, batch_size, embed_size)\n",
    "        tgt_embed = self.embedding(tgt).permute(1, 0, 2)\n",
    "        transformer_out = self.transformer(src_embed, tgt_embed)\n",
    "        output = self.fc(transformer_out).permute(1, 0, 2)  # (batch_size, seq_len, vocab_size)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== 4. Training and Evaluation ======\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, target_ids[:, :-1])  # Teacher forcing\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), target_ids[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "            output = model(input_ids, target_ids[:, :-1])\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), target_ids[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting training and validation loss\n",
    "def plot_loss(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_token_ids(token_ids, vocab):\n",
    "    \"\"\"Decode token IDs back into text using vocab mapping.\"\"\"\n",
    "    # Create reverse mapping: ID to token\n",
    "    id_to_token = {v: k for k, v in vocab.items()}\n",
    "    # Decode tokens, ignore <OOV>\n",
    "    tokens = [id_to_token[token_id] for token_id in token_ids if token_id != vocab[\"<OOV>\"]]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def calculate_bleu(model, dataloader, vocab, device):\n",
    "    \"\"\"Calculate BLEU score for model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            target_ids = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Generate predictions (adjust based on your model output)\n",
    "            outputs = model(input_ids)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Decode predictions and targets\n",
    "            for pred, target in zip(predictions, target_ids.cpu().numpy()):\n",
    "                pred_text = decode_token_ids(pred, vocab)\n",
    "                target_text = decode_token_ids(target, vocab)\n",
    "                scores.append(sentence_bleu([target_text.split()], pred_text.split(), weights=(0.5, 0.5)))\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Parameter model\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 1024\n",
    "num_heads = 8\n",
    "ff_dim = 1024\n",
    "num_layers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model, optimizer, and criterion\n",
    "model = Seq2SeqTransformer(vocab_size, embed_size, num_heads, ff_dim, num_layers).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<OOV>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate loss and backpropagate\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), target_ids[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Epochs and Batch Size\n",
    "num_epochs = 10\n",
    "batch_size = 16  # Adjust according to GPU memory\n",
    "\n",
    "# Training loop\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Progress bar for batches\n",
    "    with tqdm(total=len(train_loader), desc=\"Training Batches\", leave=False) as pbar:\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, target_ids[:, :-1])\n",
    "            \n",
    "            # Calculate loss and backpropagate\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_ids[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "# Plot loss\n",
    "plot_loss(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0002\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(model, test_loader, custom_tokenizer, vocab, device)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, input_text, tokenizer, vocab, device, max_len=50):\n",
    "    \"\"\"\n",
    "    Generate a summary for a given input text using the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input text\n",
    "        input_ids = torch.tensor(\n",
    "            [tokenizer(input_text, vocab, max_len=max_len)], dtype=torch.long\n",
    "        ).to(device)\n",
    "\n",
    "        # Start token for generation\n",
    "        start_token = vocab[\"<s>\"]\n",
    "        end_token = vocab[\"</s>\"]\n",
    "\n",
    "        # Initialize the target sequence with the start token\n",
    "        target_ids = torch.tensor([[start_token]], dtype=torch.long).to(device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Generate output probabilities\n",
    "            output = model(input_ids, target_ids)\n",
    "            next_token = torch.argmax(output[:, -1, :], dim=-1)\n",
    "\n",
    "            # Append the next token to the target sequence\n",
    "            target_ids = torch.cat([target_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Stop if end token is generated\n",
    "            if next_token.item() == end_token:\n",
    "                break\n",
    "\n",
    "        # Decode the generated token IDs into text\n",
    "        generated_summary = decode_token_ids(target_ids.squeeze().tolist(), vocab)\n",
    "        return generated_summary\n",
    "\n",
    "# Generate summaries for test data\n",
    "def predict_summaries_original(model, dataset, tokenizer, vocab, device, num_samples=10, max_len=50):\n",
    "    \"\"\"\n",
    "    Generate and display predicted summaries using original text input.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            # Load original text input and target\n",
    "            input_text = dataset[i][\"document\"]\n",
    "            target_text = dataset[i][\"summary\"]\n",
    "\n",
    "            # Tokenize the original input text\n",
    "            input_ids = torch.tensor(\n",
    "                [tokenizer(input_text, vocab, max_len=max_len)], dtype=torch.long\n",
    "            ).to(device)\n",
    "\n",
    "            # Generate summary\n",
    "            predicted_summary = generate_summary(\n",
    "                model, input_text, tokenizer, vocab, device, max_len=max_len\n",
    "            )\n",
    "\n",
    "            predictions.append({\n",
    "                \"input\": input_text,\n",
    "                \"target\": target_text,\n",
    "                \"predicted\": predicted_summary\n",
    "            })\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      "Input: Liputan6 . com , Bangka : Kapal patroli Angkatan Laut Republik Indonesia , Belinyu , baru-baru ini , menangkap tiga kapal nelayan berbendera Thailand , yakni KM Binatama , KM Sumber Jaya II , dan KM Mataram di Perairan Belitung Utara . Ketiga kapal itu ditangkap karena melanggar zona ekonomi ekslusif Indonesia . Saat ini , kapal-kapal itu diamankan di Pos Lanal Pelabuhan Pangkalan Balam , Bangka-Belitung . Menurut Komandan Pangkalan TNI AL Bangka Letnan Kolonel Laut Fredy Egam , selain menangkap tiga kapal , ALRI juga memeriksa 43 anak buah kapal . Mereka disergap saat sedang mengangkat jaring pukat harimau di Perairan Belitung Utara . Dari jumlah itu , hanya enam orang yang dijadikan tersangka , yakni tiga nahkoda dan tiga kepala kamar mesin kapal . Sedangkan ABK yang lain akan dideportasi ke negara asalnya . Meski berhasil menahan enam tersangka , TNI AL gagal mengamankan ikan tangkapan nelayan Thailand tersebut . Sebab , sebelum patroli datang , mereka telah memindahkan puluhan ton ikan hasil jaringan ke kapal induk . ( ULF/Ajmal Rokian dan Yanuar Ichrom ) .\n",
      "Target: Meski memiliki izin resmi , TNI AL tetap menangkap tiga kapan nelayan berbendera Thailand . Pasalnya , ketiga kapal itu melanggar zona ekonomi ekslusif dan menjaring ikan dengan pukat harimau .\n",
      "Predicted: <s> itu dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari\n",
      "--------------------------------------------------\n",
      "Sample 2\n",
      "Input: Liputan6 . com , Bandar Lampung : Sebanyak 51 anak di bawah umur lima tahun terserang busung lapar atau marasmus karena kekurangan gizi di Kota Madya Bandar Lampung . Lima di antaranya tewas . Data tersebut diungkapkan Kepala Dinas Kesehatan Kota Bandar Lampung M . Sudarman , baru-baru ini . Menurut Sudarman , Dinas Kesehatan Bandar Lampung mencatat sekitar 51 anak terserang busung lapar yang tersebar di beberapa kecamatan , selama periode 1999 sampai 2001 . Kebanyakan anak penderita busung tersebut berasal dari keluarga yang hidup di bawah garis kemiskinan . Selain kekurangan gizi , komplikasi radang paru-paru juga menjadi satu faktor penyebab kematian anak penderita busung lapar tersebut . Data Dinas Kesehatan menunjukkan pada 1999 , ditemukan 41 anak terserang penyakit busung lapar . Sebagian besar penderita berdomisili di kampung miskin Umbul Kunci . Jumlah penderita busung lapar menurun pada 2000 , yakni hanya sembilan anak . Sedangkan September 2001 , seorang anak meninggal karena marasmus . Sudarman menegaskan , untuk menekan jumlah korban marasmus , anak-anak dan balita diberi makanan tambahan ke sentra-sentra rawan busung lapar seperti di Desa Umbul Unci . Tetapi , berdasarkan keterangan masyarakat Umbul Kunci , program makanan tambahan ini hanya berjalan setahun yakni pada 1999 . Warga setempat mengaku tak pernah lagi menerima makanan tambahan bagi anak-anak kurang gizi sejak dua tahun terakhir . ( TNA/Bisri Merduani ) .\n",
      "Target: Sebanyak 51 anak di bawah usia lima tahun di Kota Madya Bandar Lampung , menderita busung lapar karena kekurangan gizi . Lima di antaranya meninggal dunia .\n",
      "Predicted: <s> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "--------------------------------------------------\n",
      "Sample 3\n",
      "Input: Liputan6 . com , Jakarta : Polemik seputar pelaksanaan Sidang Istimewa MPR , 1 Agustus mendatang , terus bergulir . Setelah sekian kali , Presiden Abdurrahman Wahid kembali mengeluarkan ancaman akan memberlakukan status keadaan bahaya dan darurat sipil . Langkah itu bakal dilakukan jika SI MPR tetap meminta pertanggungjawaban dirinya . Penegasan tersebut diungkapkan Presiden Wahid dalam dialog usai salat Jumat di Mesjid Al Munawarah , Ciganjur , Jakarta Selatan , Jumat ( 06/7 ) . Bagi Gus Dur , pada dasarnya dia tidak mempermasalahkan pelaksanaan SI MPR . Asalkan , agenda yang dibahas tidak menyangkut pertanggungjawaban dan kinerja pemerintahannya . \" Sebab , masih ada agenda lain yang bisa dibahas , demi kemajuan negara ini , \" kata Gus Dur , serius . Pernyataan Gus Dur kontan ditanggap Wakil Rakyat . Menurut anggota Fraksi Partai Bulan Bintang DPR Hamdan Zoelva , pemberlakuan keadaan darurat sipil justru akan mempermudah aparat keamanan untuk mengantisipasi . Jadi , siapa pun yang berniat mengganggu jalannya SI MPR , bisa diantisipasi sejak awal . Meski begitu , bagi Hamdan , saat ditemui usai dialog soal voting dan money politics Jumat siang tadi , keadaan darurat sipil tadi tak boleh salah arah . Misalnya , bila diberlakukan untuk melarang anggota dewan bersidang . \" Itu menorehkan sejarah hitam sistem ketatanegaraan kita , \" kata Hamdan . Hamdan juga menambahkan , bila darurat sipil hanya semata-mata untuk menghambat jalannya SI MPR , nilai tambah buat Gus Dur . Bahkan , tidak menutup kemungkinan malah memicu kebulatan tekad keputusan sidang Wakil Rakyat untuk menolak pertanggungjawaban Presiden Wahid . ( BMI/Tim Liputan 6 SCTV ) .\n",
      "Target: Presiden Abdurrahman Wahid kembali mengeluarkan ancaman akan memberlakukan status keadaan darurat sipil . Terutama , jika Sidang Istimewa MPR 1 Agustus mendatang tetap meminta pertanggungjawaban dirinya .\n",
      "Predicted: <s> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "--------------------------------------------------\n",
      "Sample 4\n",
      "Input: Liputan6 . com , Ambon : Bahan bakar minyak jenis solar dan premium , selama sepekan terakhir , mulai langka di Kota Ambon . Akibatnya , sebagian besar kendaraan umum di Ambon memilih berhenti beroperasi karena sulit mendapatkan bahan bakar . Selain itu , kelangkaan tersebut juga memicu kenaikan harga premium di tingkat pengecer : mencapai Rp 2 ribu per liter . Padahal , biasanya harga premium hanya Rp 1 . 700 per liter . Demikian hasil pantauan SCTV di Ambon , baru-baru ini . Beberapa pedagang minyak eceran mengaku , kelangkaan terjadi karena pasokan dari Depot Pertamina Wayame berkurang . Selain itu , mereka juga menduga kelangkaan terjadi lantaran adanya ulah oknum yang menyuplai minyak ke sejumlah kapal di Pelabuhan Maluku . Menanggapi itu , Kapala Cabang Pertamina Unit Pemasaran dan Perbekalan Dalam Negeri VIII Ambon Subaedi memastikan , kelangkaan terjadi karena adanya kebijakan Pertamina Pusat yang mengurangi jatah pasokan . Subaedi memperkirakan , kelangkaan BBM di Ambon akan terus berlangsung hingga bulan depan . Tapi , ia menolak jika kelangkaan terjadi akibat adanya kerja sama kotor antara Pertamina dan para spekulan . ( ICH/Sahlan Heluth ) .\n",
      "Target: Sepekan terakhir , bahan bakar minyak jenis solar dan premium di Kota Ambon , mulai langka . Kelangkaan terjadi karena pasokan BBM dari Depot Pertamina Wayame berkurang .\n",
      "Predicted: <s> ke . Namun menjadi dan tak bisa . Namun menjadi dan tak bisa . Namun menjadi dan tak akan dan tak akan dan tak akan dan tak akan dan tak akan dan dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari dari\n",
      "--------------------------------------------------\n",
      "Sample 5\n",
      "Input: Liputan6 . com , Jakarta : Seluruh perubahan pasal Undang-Undang Nomor 23 tahun 1999 tentang Bank Indonesia telah disetujui oleh pemerintah dan DPR . Namun masih ada satu polemik soal pasal 75 . Inti pasal itu adalah soal memberhentikan seluruh anggota Dewan Gubernur BI , setelah UU Amendemen BI berlaku efektif . Bagi BI sendiri , posisinya harus tetap independen . Pernyataan itu ditegaskan Gubernur BI Sjahril Sabirin , baru-baru ini , di Jakarta . Menurut Sjahril , persoalan amendemen sudah menjadi kewenangan pemerintah dan DPR . Sebab kedua lembaga tersebut telah mendapat masukan dari tim panel Dana Moneter Internasional ( IMF ) . \" IMF sudah menganjurkan pembentukan tim panel , \" kata Sjahril . Lantas , tambah dia , tim panel itu juga sudah mengeluarkan hasil keputusan . \" Sekarang , tergantung pemerintah dan DPR untuk menerapkannya , \" kata Sjahril , serius . Pembahasan pasal 75 sendiri masih berlangsung di Panitia Kerja DPR . Panja ini mengusulkan pilihan supaya pemerintah mempercepat pembentukan Dewan Supervisi BI . Lantas , panja juga menawarkan kepada pemerintah untuk membuat laporan mengenai penyimpangan di tubuh BI . Berdasarkan laporan ini , DPR mesti menggelar paripurna untuk menentukan perlu tidaknya mengganti Gubernur BI . ( COK/Olivia Rosalia dan Bambang Triono ) .\n",
      "Target: Bank Indonesia menyerahkan sepenuhnya amendemen UU BI tentang pergantian dewan gubernur kepada pemerintah dan DPR . Diperkirakan , polemik soal pasal 75 sulit untuk bisa mencapai kata sepakat .\n",
      "Predicted: <s> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Liputan6 dataset\n",
    "dataset = load_dataset(\"SEACrowd/liputan6\", trust_remote_code=True)\n",
    "\n",
    "# Select the test set\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict_summaries_original(model, test_data, custom_tokenizer, vocab, device, num_samples=5)\n",
    "\n",
    "# Display predictions\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Sample {i + 1}\")\n",
    "    print(f\"Input: {pred['input']}\")\n",
    "    print(f\"Target: {pred['target']}\")\n",
    "    print(f\"Predicted: {pred['predicted']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Liputan6 . com , Bangka : Kapal patroli Angkatan Laut Republik Indonesia , Belinyu , baru-baru ini , menangkap tiga kapal nelayan berbendera Thailand , yakni KM Binatama , KM Sumber Jaya II , dan KM Mataram di Perairan Belitung Utara . Ketiga kapal itu ditangkap karena melanggar zona ekonomi ekslusif Indonesia . Saat ini , kapal-kapal itu diamankan di Pos Lanal Pelabuhan Pangkalan Balam , Bangka-Belitung . Menurut Komandan Pangkalan TNI AL Bangka Letnan Kolonel Laut Fredy Egam , selain menangkap tiga kapal , ALRI juga memeriksa 43 anak buah kapal . Mereka disergap saat sedang mengangkat jaring pukat harimau di Perairan Belitung Utara . Dari jumlah itu , hanya enam orang yang dijadikan tersangka , yakni tiga nahkoda dan tiga kepala kamar mesin kapal . Sedangkan ABK yang lain akan dideportasi ke negara asalnya . Meski berhasil menahan enam tersangka , TNI AL gagal mengamankan ikan tangkapan nelayan Thailand tersebut . Sebab , sebelum patroli datang , mereka telah memindahkan puluhan ton ikan hasil jaringan ke kapal induk . ( ULF/Ajmal Rokian dan Yanuar Ichrom ) .\n",
      "\n",
      "Tokenized IDs:\n",
      "[4, 5, 6, 7, 9645, 9, 6398, 8901, 4128, 3112, 2955, 380, 7, 1, 7, 800, 30, 7, 1120, 1378, 568, 1096, 1, 8233, 7, 64, 6423, 1, 7, 6423, 2756, 923, 1743, 7, 58, 6423, 12958, 117, 7577, 9646, 1172, 5, 4836, 568, 193, 4602, 182, 4301, 6238, 1678]\n",
      "\n",
      "Tokenized Tokens:\n",
      "['Liputan6', '.', 'com', ',', 'Bangka', ':', 'Kapal', 'patroli', 'Angkatan', 'Laut', 'Republik', 'Indonesia', ',', '<unk>', ',', 'baru-baru', 'ini', ',', 'menangkap', 'tiga', 'kapal', 'nelayan', '<unk>', 'Thailand', ',', 'yakni', 'KM', '<unk>', ',', 'KM', 'Sumber', 'Jaya', 'II', ',', 'dan', 'KM', 'Mataram', 'di', 'Perairan', 'Belitung', 'Utara', '.', 'Ketiga', 'kapal', 'itu', 'ditangkap', 'karena', 'melanggar', 'zona', 'ekonomi']\n",
      "\n",
      "Padded Sequence:\n",
      "[4, 5, 6, 7, 9645, 9, 6398, 8901, 4128, 3112, 2955, 380, 7, 1, 7, 800, 30, 7, 1120, 1378, 568, 1096, 1, 8233, 7, 64, 6423, 1, 7, 6423, 2756, 923, 1743, 7, 58, 6423, 12958, 117, 7577, 9646, 1172, 5, 4836, 568, 193, 4602, 182, 4301, 6238, 1678]\n"
     ]
    }
   ],
   "source": [
    "def inspect_tokenization(tokenizer, vocab, text, max_len=50):\n",
    "    \"\"\"\n",
    "    Inspect the tokenization, sequence, and padding for a given text.\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\")\n",
    "    print(text)\n",
    "    print(\"\\nTokenized IDs:\")\n",
    "    token_ids = tokenizer(text, vocab, max_len=max_len)\n",
    "    print(token_ids)\n",
    "\n",
    "    print(\"\\nTokenized Tokens:\")\n",
    "    id_to_token = {v: k for k, v in vocab.items()}\n",
    "    tokens = [id_to_token.get(token_id, \"<UNK>\") for token_id in token_ids]\n",
    "    print(tokens)\n",
    "\n",
    "    print(\"\\nPadded Sequence:\")\n",
    "    print(token_ids)\n",
    "\n",
    "# Example usage with one sample from the dataset\n",
    "sample = test_data[0]  # Replace with your dataset object\n",
    "input_text = sample[\"document\"]\n",
    "\n",
    "inspect_tokenization(custom_tokenizer, vocab, input_text, max_len=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
